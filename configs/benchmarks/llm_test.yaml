game_name: "llm_10p_3model"
num_players: 10

roles:
  - role: werewolf
    count: 3
  - role: seer
    count: 1
  - role: doctor
    count: 1
  - role: villager
    count: 5

default_model:
  api_base: "http://localhost:11434/v1"
  api_key: "ollama"
  model: "llama3.2:3b"
  reasoning_temperature: 0.7
  action_temperature: 0.3
  timeout: 120.0
  max_tokens: 2048
  context_length: 16384

players:
  # 4 on llama3.2:3b
  - name: Alice
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "llama3.2:3b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  - name: Bob
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "llama3.2:3b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  - name: Charlie
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "llama3.2:3b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  - name: Diana
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "llama3.2:3b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  # 4 on gpt-oss:20b
  - name: Eve
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "gpt-oss:20b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  - name: Frank
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "gpt-oss:20b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  - name: Grace
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "gpt-oss:20b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  - name: Hank
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "gpt-oss:20b"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 120.0
      max_tokens: 2048
      context_length: 16384
  # 2 on qwq:32b -- needs high max_tokens for <think> chain-of-thought
  - name: Ivy
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "qwq:32b-q4_K_M"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 300.0
      max_tokens: 4096
      context_length: 16384
  - name: Jack
    model:
      api_base: "http://localhost:11434/v1"
      api_key: "ollama"
      model: "qwq:32b-q4_K_M"
      reasoning_temperature: 0.7
      action_temperature: 0.3
      timeout: 300.0
      max_tokens: 4096
      context_length: 16384

voting:
  method: "plurality"
  reveal_votes: true
  tie_breaker: "random"

communication:
  allow_wolf_chat: true
  allow_dms: false
  discussion_rounds: 1
  max_speech_length: 300

max_days: 5
